ðŸ“Œ Note:
This task is part of a structured internship to reinforce basic ML skills.
Itâ€™s not meant to reflect my current skill level or portfolio quality.

# Task 01 - Data Cleaning & Preprocessing

This task is part of a structured AI/ML internship designed to reinforce fundamentals through hands-on practice.

## Objective
Perform exploratory analysis and preprocessing (null handling, encoding, scaling, outlier removal) on the Titanic dataset to prepare it for machine learning.

## Highlights
- Practical preprocessing with real-world dataset
- Feature encoding (Label & One-Hot)
- Outlier removal using IQR method
- Scaling via StandardScaler
- Code written modularly for reuse

## Dataset
[Kaggle Titanic Dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset)

## Tools Used
Python, Pandas, NumPy, Seaborn, Scikit-learn, Matplotlib


## ðŸš€ How to Use the Code

1. **Open the notebook in Google Colab**  
   - Upload the `Titanic-Preprocessing.ipynb` file to [Google Colab](https://colab.research.google.com/)

2. **Attach the dataset**  
   - The dataset file `Titanic-Dataset.csv` is already included in this repository  
   - Upload it when prompted by the notebook (using the file picker in Colab)

3. **Run all cells**  
   - The notebook will:
     - Load the data
     - Handle missing values
     - Encode categorical variables
     - Scale numerical features
     - Visualize and optionally remove outliers
     - Save the cleaned dataset as `titanic_cleaned.csv`

4. **Download cleaned data (optional)**  
   - The cleaned file will be made available for download at the end

